\documentclass[preprint,nocopyrightspace]{sig-alternate}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{hyperref}

% standard packages that must be loaded after hyperref
\usepackage[auth-lg]{authblk}
\usepackage{bookmark}
\usepackage{booktabs}
\usepackage[final]{listings}
\usepackage{lscape}
\usepackage{mathtools}
\usepackage{paralist}
\usepackage{flushend}

% local packages for just this paper
\usepackage{natbib-cite}
\usepackage{natbib-acm}

% packages that must be loaded after both hyperref and natbib
\usepackage{hypernat}
\usepackage{cleveref}

\usepackage{booktabs}
\usepackage{caption}

\crefname{section}{Section}{Sections}
\crefname{table}{Table}{Tables}
\crefname{figure}{Figure}{Figures}
\crefname{subfigure}{Figure}{Figures}

\begin{document}%

\title{Practical Property-based testing in scientific software}

\author{Mousumi Chattaraj}
\author{Luiz Irber}
\author{Li Li}

\affil{\normalsize{University of California, Davis}\\
\{\texttt{mchattaraj,lcirberjr,llili\}@ucdavis.edu}}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
Correct software is expensive to develop.
Scientific software suffers even more due to lack of software development expertise among scientists and maintainability problems:
people leave labs,
usually without creating enough documentation or processes for continued development.
We would like to explore ways to increase our confidence in the correctness of scientific software or,
more realistically,
find cheap ways of show when it is incorrect.

The khmer project is a mixed Python/C++ codebase implementing a few core data structures (sketches) and many methods and analysis on top of it.
The source code is available on GitHub under a 3-clause BSD license,
and every pull request is tested on a Continuous Integration server (Jenkins).
It has high coverage (90\% line coverage) derived from manually written (~600) unit tests,
but many tests were written just to trigger coverage,
with no particular focus on correctness.

According to \citet{claessen_quickcheck:_2011},
passing numerous tests in trivial cases can create a false sense of security,
and many of the unit tests currently in khmer fits the description.
One technique available for better testing is property-based testing,
where instead of testing for specific cases the test is generalized (in terms of properties),
and a library generates inputs to try to falsify these properties.
If every property is verified the software cannot be considered as correct and fully tested,
but it does provide stronger evidence it is not wrong in complex cases.

\section{Motivating Example}

Hashing is an essential operation in khmer,
since all the core data structures depend on good hash functions to perform correctly.
Benefiting from a public development process coordinated through GitHub,
an exploratory branch was created with an accompanying issue and discussion:
\url{https://github.com/dib-lab/khmer/issues/990}

This original exploration used a property from the hash function,
reversibility:
any string used as input generates a hash value (an integer),
which can be used to generate the original string again.
The implementation using Hypothesis is the following:

\begin{lstlisting}[language=Python,basicstyle=\small\tt,caption={Motivating example: testing a reversible hash function},label={revhash}]
@given(text("ACGT"))
def test_forward_hash_no_rc(kmer):
    ksize = len(kmer)
    assume(ksize > 0)
    assume(ksize < 32)

    rh = reverse_hash(
        forward_hash_no_rc(kmer, ksize),
        ksize)
    assert rh == kmer
\end{lstlisting}

This example contains some important aspects:
\begin{enumerate}
\item the input $kmer$ is generated automatically from a specification (the $given$ decorator),
meaning each $kmer$ is a string with alphabet "ACGT".
\item the $assume$ function asserts that the generated $kmer$ is not an empty string,
and also not longer than 32 characters.
This is used by Hypothesis to guide the input generation.
\item the conditions are tested using normal Python syntax (assert, variables, function calls)
\end{enumerate}

The input specification was added later,
because the code was not prepared to deal with empty strings,
strings longer than 32 characters or with a different alphabet.
These are all bugs that need to be fixed,
and they are good to show the approach.
This example is very similar to typical unit testing in Python,
while the Hypothesis extensions to the concept allow both property-based testing and fuzzing usage.

\section{Technical Approaches}

We extended the available infrastructure for running tests in khmer to also run our new tests.
During development we also set up Travis CI for continuous integration and Coveralls for coverage reports
(both for convenience,
because khmer already have a Jenkins CI server up and running,
but it is not as easy to track coverage in it).
Every change committed to the repository is tested,
generating coverage data (in \emph{gcov} format) and uploaded to Coveralls for analysis.
Since Coveralls doesn't support function coverage,
we also set up LCOV and uploaded the generated report to another server.
Links for the reports are available in the project repository README:
\url{https://github.com/luizirber/ecs260-property/}

\subsection{Incremental exploration}
Our plan is to understand what are the best options to generate meaningful Hypothesis tests to find incorrect behavior in khmer.
We'll do progressive coverage and exploration of the khmer codebase,
starting from simpler pieces and building up knowledge to be able to tackle complex regions.
This allows us to start from valid input (to figure out what properties make sense to be tested),
and then increase the complexity of the code by generating invalid inputs too.
Most tests will deal with biological data in the form \emph{sequences},
strings on a restricted alphabet "ACGT",
or \emph{records} composed of a \emph{name} (initially restricted to printable ASCII characters), \emph{sequence} and optionally a \emph{quality score} for each character in the \emph{sequence}.
We can increase the complexity (and the validity) of inputs by allowing extra characters in any of the fields of a record.
In the extreme case,
after the code is sufficiently resilient,
we can also start generating unustructured random data,
turning the whole approach into a fuzzing test.
Fuzzing \citet{qi2013research} is a technique introduced in 1990 which found failures in UNIX utilities by generating random inputs,
but it doesn't help with correctness verification,
only to find crashing bugs.

We split the plan into four areas:

\subsubsection{Hashing}
Hashing is the implementation of functions mapping strings to an integer space.
Depending on the implementation this operation is reversible.

\subsubsection{HyperLogLog}
HyperLogLog is an algorithm that finds the number of unique elements in a multiset.
It is a probabilistic cardinality estimator,
consuming less memory than exact solutions.
The algorithm uses a hash function in the original multiset to produce uniformly-distributed random numbers that have the same cardinality \citet{hyperloglog_2007}.
Usually a $set$ can be used as an oracle in tests involving HyperLogLog,
since the cardinality estimation is an operation equivalent to the size of the set.

\subsubsection{Bloom Filter}
A Bloom Filter is a data structure designed to check the presence of an element in a set rapidly and memory-efficiently.
A bit vector is the base structure of a bloom filter,
where each empty cell represents a bit and the number below is its index. \citet{bloom_filter}
In order to add an element to the table,
we will hash it a few times and then set the bits in the bit vector at the index of those hashes to 1.
False positive matches are possible,
but false negatives are not.
A $set$ is the ideal oracle for a Bloom Filter.

\subsubsection{Count-Min Sketch}
Count-Min Sketch is a probabilistic data structure that projects events to frequencies with the implementation of hash functions.
The output is a frequency table \citet{count_min_sketch}.
The goal is to consume a stream of events one at a time,
and count the frequency of different types of events in it without storing the whole stream \citet{count_min_sketch_2005}.
In the Python standard library there is a class,
\emph{collection.Counter},
that can be used as an oracle for frequency counting.

\subsection{Unit tests adaptation}
Many unit tests for khmer are based on fixed values,
and checking the results for this specific value.
Hypothesis has a decorator,
\emph{@example},
that can be used to run specific inputs.
We can use this to to prototype new tests:
by copying the unit test and setting the example,
we have a working test and then can generalize for some property.

\section{Evaluation Methodology}
It is hard to find a good metric for success in this project:
number of bugs found wouldn't necessarily represent interesting or important bugs,
but at the same time we can't guarantee we'll find any non-trivial bug.
There is also the complexity of reasoning about the data structures to find testable properties and how to set up the tests.
Nonetheless,
we would like to track how many method calls we can cover,
starting from the core data structures and moving to specialized methods (the ones used directly by data analysis scripts).
We're not aiming at complete line or branch coverage,
but it is likely that we can get good coverage with the Hypothesis tests,
even if inconsistent between runs because new test cases will be generated.

Coveralls is a web service to help track the coverage of our code,
and since all the code is hosted on GitHub it is easy to integrate it to the workflow.
We are then using Travis CI to get a report on the coverage level.
Travis can be enabled by the click of a button from GitHub repository.
We are also using LCOV to report function coverage,
because Coveralls doesn't support it.
LCOV is a graphical front-end for GCC's coverage testing tool gcov.
It collects data from gcov for more than one source file and creates web pages containing the
source code with coverage details.
It also adds pages for easy navigation in the file structure.
LCOV supports line coverage, function coverage and branch coverage measurement.

Finally,
we want to document and generate guidelines for testing similar software,
as well as other biological data analysis projects or scientific in general.
This document can be used as a quickstart and introduction to these testing techniques in specialized fields.

\section{Experimental Results}

We wrote 16 property tests,
with at least two tests covering each of the areas proposed for the incremental exploration.
These tests were enough to generated a method coverage of $37.9\%$ in the C++ code (the \emph{lib} directory),
as shown in table \ref{table:lcov}.
The \emph{khmer} directory contains the glue code for calling C++ code from Python,
and while important it wasn't part of our initial focus.

\noindent
\begin{minipage}{\columnwidth}
  \captionsetup{type=table}
  \centering
  \begin{tabular}{|c|c|c|}
  \toprule
  Directory & Line Coverage & Functions \\
  \midrule
  khmer		& 14.0\% (217/1554)	& 19.5\% (29/149) \\
  lib		& 19.1\%(580/3042)	& 37.9\% (96/253) \\
  Total		& 17.3\% (797/4596)	& 31.1\% (125/402) \\
  \bottomrule
  \end{tabular}
  \medskip

  \caption{Line and function coverage information from LCOV.}\label{table:lcov}
\end{minipage}

\subsection{Interesting bugs found}

\subsubsection{The HyperLogLog cardinality estimation is out of the error bounds}

When running the test for HyperLogLog cardinality estimation,
a total error rate of 1\% or less is expected.
We set up the test to also consider when less than one hundred unique elements are tested,
because over or underestimating by 1 or 2 is still acceptable (we want to estimate large cardinalities).
However,
even considering the hashing function is not perfectly uniform (and so smalls variations on the error rate are expected),
Hypothesis found falsifying examples for both low cardinality cases (10 items in the set, 7 in the HyperLogLog) and large cardinalities (where it was closer to 3\%.

\begin{lstlisting}[language=Python,basicstyle=\small\tt,caption={HyperLogLog cardinality estimation test},label=hlltest]
@given(st.lists(st_kmer,
                min_size=10,
                unique_by=lex_rc))
def test_hll_cardinality(kmers):
    oracle = set()
    hll = khmer.HLLCounter(0.01, KSIZE)

    for kmer in kmers:
        oracle.update([kmer])
        hll.consume_string(kmer)

    if len(oracle) < 100:
        assert abs(len(oracle) - len(hll)) <= 2
    else:
        error = (abs(len(hll) - len(oracle))
                 / len(oracle))
        assert error <= 0.02
\end{lstlisting}


The falsifying example can now be used to understand why the estimation is not behaving as expected,
but it will demand more analysis to figure this.

\begin{lstlisting}[language=Bash,basicstyle=\tiny\tt,caption={HyperLogLog cardinality estimation test output, with falsifying example},label=hlltestoutput]
======================================================================
FAIL: Testing HyperLogLog cardinality estimation.
----------------------------------------------------------------------
Traceback (most recent call last):
  (...)
  File "test_hypothesis.py", line 230, in test_hll_cardinality
    assert abs(len(oracle) - len(hll)) <= 2, (len(oracle), len(hll))
AssertionError: (10, 7)
-------------------- >> begin captured stdout << ---------------------
Falsifying example: test_hll_cardinality(kmers=['CTTCGTCCTTGTC',
'CCTTGCTTTTTGT', 'GCCGCGCTGTGCT', 'GCGTTTTCGTTTC', 'AAAAAAAAAAAAA',
'AAACAAAAAAAAA', 'AAAAAAAAACAAA', 'AAAAAAAAAAACA', 'ACAAAAAAAAAAA',
'CTCTCTCCCTTCC'])
--------------------- >> end captured stdout << ----------------------
\end{lstlisting}

\subsubsection{Introducing invalid data, finding inconsistent behavior}

All tests are still using only valid data as input,
except one: \emph{test\_countgraph\_consume\_fasta}.
We used this test to start implementing invalid input generation,
and it was an appropriate choice because \emph{Countgraph} (the library name for Count-Min sketches)
implements both a \emph{consume\_fasta} method (for reading multiple records from a file) and \emph{consume},
which consumes the sequence from only one record.
We allow invalid records to be generated by accepting any Unicode character,
either for names or sequences.
Because generating long lists of records without errors would be very difficult if we chose each element between valid and invalid,
we opted for generating either a list with only valid records (same behavior from initial implementation),
or generating lists with invalid records (which might have valid records too, but rarely).
All these options can be controlled by using search strategies defined by default on Hypothesis:
\emph{one\_of} and \emph{lists}.
\emph{st\_record} and \emph{st\_invalid\_records} are the strategies we defined using basic default strategies to generate both valid and invalid records.

\begin{lstlisting}[language=Python,basicstyle=\tiny\tt,caption={Countgraph sequence consumption test},label={CountgraphTest}]
@given(st.one_of(st.lists(st_record),
                 st.lists(st_invalid_record)))
def test_countgraph_consume_fasta(records):
    cg_fasta = khmer.Countgraph(KSIZE, TABLE_SIZE, N_TABLES)
    cg_string = khmer.Countgraph(KSIZE, TABLE_SIZE, N_TABLES)

    fasta = fasta_build(records)
    with NamedTemporaryFile() as temp:
        temp.write(fasta.encode('utf-8'))
        temp.flush()

        try:
            cg_fasta.consume_fasta(temp.name)
        except OSError:
            assert fasta == ''

    for record in records:
        cg_string.consume(record['sequence'])

    assert cg_fasta.n_unique_kmers() == cg_string.n_unique_kmers()
    assert cg_fasta.n_occupied() == cg_string.n_occupied()
    assert cg_fasta.hashsizes() == cg_string.hashsizes()
\end{lstlisting}

Surprisingly,
we didn't even have to finish implementing the property checks to find a bug:
during prototyping we hit an error and we found inconsistencies in the behavior of both functions.
\emph{consume\_fasta} accepted an invalid record without errors,
but when \emph{consume} was executed on the same record a validation error was reported.

\begin{lstlisting}[language=Bash,basicstyle=\tiny\tt,caption={Inconsistent behavior between consume\_fasta and consume},label={CountgraphConsumeOutput}]
======================================================================
ERROR: tests.test_hypothesis.test_countgraph_consume_fasta
----------------------------------------------------------------------
Traceback (most recent call last):
  (...)
  File "tests/test_hypothesis.py", line 199, in test_countgraph_consume_fasta
    cg_string.consume(record['sequence'])
ValueError: string length must >= the hashtable k-mer size
-------------------- >> begin captured stdout << ---------------------
Falsifying example: test_countgraph_consume_fasta(records=[
		{'name': '0', 'sequence': '0'}])
--------------------- >> end captured stdout << ----------------------
\end{lstlisting}

The question is: Which behavior is correct? \emph{consume\_fasta} ignored invalid records,
but this should probably be controlled with a parameter for the method,
instead of silently ignoring records.
\emph{consume} raised an exception,
but depending on the analysis being done we might want to just ignore errors instead of crashing the program.

\subsection{Generalizing existing unit tests}

Because we already have many unit tests available for khmer,
we wanted to figure out how to leverage this knowledge to make it easier to write Hypothesis test.
As an example,
we show here how we converted the unit test for the \emph{Nodegraph.filter\_if\_present} method into a Hypothesis test.

The \emph{filter\_if\_present} unit test takes two previously generated files,
\emph{filter-test-A.fa} and \emph{filter-test-B.fa'},
and reads the first one as a mask using \emph{consume\_fasta}.
\emph{Nodegraph.filter\_if\_present} then takes the second file as input,
and filter out any sequence containing \emph{k-mers} from the first file,
writing the output to a third file.
Finally,
this final file is read as records and two specific values are checked:
if only one record remained,
and if the record name is '3'.

\begin{lstlisting}[language=Python,basicstyle=\tiny\tt,caption={filter\_if\_present unit test},label={FilterUnit}]
def test_filter_if_present():
    nodegraph = khmer._Nodegraph(32, [3, 5])

    maskfile = utils.get_test_data('filter-test-A.fa')
    inputfile = utils.get_test_data('filter-test-B.fa')
    outfile = utils.get_temp_filename('filter')

    nodegraph.consume_fasta(maskfile)
    nodegraph.filter_if_present(inputfile, outfile)

    records = list(screed.open(outfile))
    assert len(records) == 1
    assert records[0]['name'] == '3'

\end{lstlisting}

While useful to check if it works in this case,
the test can be generalized as follows:

\begin{enumerate}
\item generate two lists of records: \emph{mask} and \emph{records}
\item write both lists to files: the \emph{mask\_fasta} file contains all records from the \emph{mask} list,
while the \emph{input\_fasta} file contains all sequences (from both \emph{mask} and \emph{records}).
\item instead of checking for specific values in the filtered records,
write an assertion verifying if all sequences present in \emph{mask} are absent in the filtered records.
\item finally, use the \emph{@example} decorator to list the records from the original files from the unit test,
guaranteeing that we are still checking the initial case every time.
\end{enumerate}

While more verbose due to file manipulation,
the following code implement all items from the list and can be executed for any generated input:

\begin{lstlisting}[language=Python,basicstyle=\tiny\tt,caption={filter\_if\_present Hypothesis test},label={FilterHypothesis}]
@given(st.lists(st_record), st.lists(st_record))
@example([{"name": "a", "sequence": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"}],
         [{"name": "1", "sequence": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"},
          {"name": "2", "sequence": "GAGATCAGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"},
          {"name": "3", "sequence": "AGAGATACACAAGATAGAGAGACCCAGGAGGGGG"}])
def test_nodegraph_filter_if_present(mask, records):
    ng = khmer.Nodegraph(KSIZE, TABLE_SIZE, N_TABLES)

    mask_fasta = fasta_build(mask)
    with NamedTemporaryFile() as maskfile:
        maskfile.write(mask_fasta.encode('utf-8'))
        maskfile.flush()

        try:
            ng.consume_fasta(maskfile.name)
        except OSError:
            assert mask_fasta == ''

    input_fasta = fasta_build(records)
    with NamedTemporaryFile() as outfile:
        with NamedTemporaryFile() as inputfile:
            inputfile.write(mask_fasta.encode('utf-8'))
            inputfile.write(input_fasta.encode('utf-8'))
            inputfile.flush()

            try:
                ng.filter_if_present(inputfile.name, outfile.name)
            except OSError:
                assert input_fasta == ''

        with screed.open(outfile.name) as filtered:
            filtered_records = [r for r in filtered]

    assert all(m['sequence'] not in r['sequence']
               for m in mask
               for r in filtered_records)

\end{lstlisting}

%\subsection{Initial strategies for biological sequence data}

%\subsection{Practical guidelines for property-based testing}

\section{Related Work}

\subsection{QuickCheck}
Property-based testing has its roots on the Haskell programming language,
in a module called QuickCheck (\citet{claessen_quickcheck:_2011}).
Instead of defining test cases for specific values (like corner cases),
QuickCheck proposes that tests should be verifiable by checking properties and invariants that describe what is expected for any input,
and then generating inputs to try to falsify the test.
Haskell is a functional language that avoids side-effects ("pure"),
making it simpler to reason about properties as mathematical aspects,
and apply concepts across many domains.
This approach doesn't work so well in other languages,
and for Python there is a library called Hypothesis implementing a mix of unit testing,
property-based testing,
and fuzzing.

\subsection{QuickCheck: A Lightweight Tool for Random Testing of Haskell Programs}
QuickCheck(\citet{claessen_quickcheck:_2011}) assists in testing program properties (Which are Haskell functions) for Haskell programmer.
The properties can be tested automatically on random input or custom set data.
Testing is a popular and expensive approach to check software quality.
Reducing the cost of testing has been a major motivation to automate the process.
Two relatively old ideas, oracles and random testing are combined and work well for Haskell.
The first step is defining properties, with features like quantifiers, conditionals and data monitors.
Next step is defining generators, which are type-based default random test data generators.
The last step is providing an embedded language to specify custom test data generators.
It has also been proven that the functional nature allowed for local and fine-grained properties,
and this tool is lightweight and easy to use.

\subsection{Hypothesis}
Hypothesis (\citet{maciver_welcome_2015}) is a Python library for creating simple and powerful unit tests to find edge cases.
It asks user to write cases that assert something should be true for every case instead of just the ones that could be thought of.
For a normal unit test,
the first step is to setup some data,
and then performing some operations on the data,
and at last asserting something about the result.
However Hypothesis asks user to write test cases for all data matching some specification,
and then performs some operation on the data,
and makes the assertion in the end.
This is often called property based testing.
Hypothesis works by generating random data that match the specification for the data and checks if the guarantee still holds in the case.
If not it will take the example and cut its size to simplify it until a much smaller example that still causes the problem is found.
Hypothesis then saves that example for later use.

\subsection{khmer}
khmer (\citet{crusoe_khmer_2015}) is a free software library that works with fixed length DNA words (also known as $k$-mers) and implements a probabilistic k-mer counting data structure.
Currently khmer is implemented in $C++$ and Python.
The data structures and graph traversal code are implemented in $C++$,
and then wrapped for Python in hand-written C code.
khmer is primarily developed on Linux for Python 2.7 and 64-bit processors,
and several core developers use Mac OS X.
$k$-mers are a common abstraction in DNA sequence analysis that enable alignment-free sequence analysis and comparison.
However the dramatic increase in sequence data generation has led to the development of data structures ad algorithms to discover possible improvements to $k$-mer-based approaches.
The khmer project provides several efficient data structures and algorithms to analyze short-read nucleotide sequencing data while emphasizing online analysis,
low memory data structures and streaming algorithms.

\subsection{The economics of software correctness}
According to \citet{maciver_economics_2015},
it is almost impossible for someone to write a significant piece of correct software.
It is reasonable to write bug-free small libraries,
but the chances to write a whole perfect program are almost zero.
The problem is not that people do not know how to write correct software,
it is that correct software is too expensive.
Being too expensive means that nobody would be willing to pay a price for the software,
or a competing product could be released much earlier.
The result of this is shipping buggy software to users.
The problem is:
bugs found by users are more expensive than bugs found before a user sees them,
since in the former case the result could be lost users, lost time and etc.
Also, fixing a production bug needs more analysis since
there can not be extended down time for production system.
Hypothesis is a good example of reducing the effort of finding bugs.

\subsection{Automatic Predicate Abstraction of C Programs}
Model checking has been proven very efficient in testing hardware and protocol domains.\citet{model_checking}
The increase of interest in the implementation of model checking in software has led to the development of C2BP,
a tool that combines predicate abstraction, model checking, symbolic reasoning, and iterative refinement to test a program.
C2PB is able to perform predicate abstraction of C programs automatically.
It produces boolean output to the BEBOP model checker to compute the set of reachable states.
C2BP is part of the SLAM tool kit, which goal is to check a programâ€™s temporal safety properties automatically.

\section{Conclusion}



\bibliography{property}

\end{document}
